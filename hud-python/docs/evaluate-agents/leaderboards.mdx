---
title: "Leaderboards"
description: "Track and compare agent performance on standardized benchmarks"
icon: "trophy"
---

HUD automatically tracks all benchmark runs and provides leaderboards for comparing agent performance across teams and models.

## Running Benchmarks

Execute evaluations on standard datasets:

```python
from hud.datasets import run_dataset_parallel
from hud.agents import ClaudeAgent

# Run a benchmark (parallel process execution for performance)
results = await run_dataset_parallel(
    name="Claude-3.5 SheetBench",
    dataset="hud-evals/sheetbench-50",
    agent_class=ClaudeAgent,
    agent_config={"model": "claude-3-5-sonnet-20241022"}
    # Automatically optimizes worker count
)
```

## Viewing Results

### On the Leaderboard Page

After running, view your results:

1. Navigate to `app.hud.so/leaderboards/{dataset-name}`
   - Example: `app.hud.so/leaderboards/hud-evals/sheetbench-50`
2. Click "My Jobs" to see your evaluation runs
3. Select runs to analyze performance

### Local Analysis

Calculate metrics locally:

```python
# Basic metrics
success_rate = sum(r.reward > 0.5 for r in results) / len(results)
avg_reward = sum(r.reward for r in results) / len(results)
avg_duration = sum(r.duration for r in results) / len(results)

print(f"Success Rate: {success_rate:.2%}")
print(f"Average Reward: {avg_reward:.3f}")
print(f"Average Duration: {avg_duration:.1f}s")
```

## Creating Scorecards

Transform your evaluation run into a public scorecard:

<Frame>
  <img src="/images/create-scorecard.png" alt="Creating a scorecard from evaluation results" />
</Frame>

<Steps>
<Step title="Run Evaluation">
  Execute your agent on the full dataset
</Step>

<Step title="Navigate to Leaderboard">
  Go to `app.hud.so/leaderboards/{dataset-name}`
</Step>

<Step title="Select Your Job">
  Click "My Jobs" and choose your evaluation run
</Step>

<Step title="Create Scorecard">
  Select your best runs (we compute statistics over multiple jobs) and create a scorecard
</Step>

<Step title="Publish">
  Your scorecard appears on the public leaderboard
</Step>
</Steps>

<Frame>
  <img src="/images/leaderboard-example.png" alt="Example leaderboard showing different models and their scores" />
</Frame>

## Available Benchmarks

<CardGroup cols={2}>
<Card title="SheetBench-50" href="https://app.hud.so/leaderboards/hud-evals/SheetBench-50">
  Spreadsheet manipulation and analysis tasks
</Card>

<Card title="OSWorld-Verified" href="https://app.hud.so/leaderboards/hud-evals/osworld-verified">
  Desktop automation in Ubuntu environment
</Card>

<Card title="2048 Puzzles" href="https://app.hud.so/leaderboards/hud-evals/2048-puzzles">
  Strategic planning and game playing
</Card>

<Card title="OnlineMind2Web" icon="clock">
  Web navigation challenges (Coming Soon)
</Card>
</CardGroup>

## Best Practices

1. **Agent Naming**: Ensure your agent class has a meaningful name - it will be displayed on the leaderboard
2. **Multiple Runs**: Select your best few runs when creating a scorecard (we compute statistics over them)
3. **Consistent Configuration**: Use the same model version and parameters for fair comparison
4. **Share Results**: Make your scorecards public to contribute to the community

<Note>
Leaderboards are automatically created when the first scorecard is published for a dataset
</Note>

## Next Steps

<CardGroup cols={2}>
<Card title="Create Agents" icon="robot" href="/evaluate-agents/create-agents">
  Build your own MCP-compatible agent
</Card>

<Card title="Create Benchmarks" icon="flask" href="/evaluate-agents/create-benchmarks">
  Build custom evaluation datasets
</Card>
</CardGroup>