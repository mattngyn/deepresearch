---
title: "Evaluate Agents"
description: "Test and benchmark your agents on standardized tasks"
icon: "flask-vial"
---

HUD makes it easy to evaluate any MCP-compatible agent on a variety of tasks and benchmarks. Whether you're testing Claude, Operator, or your custom agent, the evaluation flow is consistent.

## Quick Start

Evaluate an agent on a single task:

```python
import hud
import os
from hud.datasets import Task
from hud.agents import ClaudeAgent

with hud.trace("eval-demo"):
    task = Task(
        prompt="Create a spreadsheet with quarterly sales data",
        mcp_config={
            "hud": {
                "url": "https://mcp.hud.so/v3/mcp",
                "headers": {
                    "Authorization": f"Bearer {os.getenv('HUD_API_KEY')}",
                    "Mcp-Image": "hudpython/hud-browser:latest"
                }
            }
        },
        setup_tool={
            "name": "playwright",
            "arguments": {
                "action": "navigate",
                "url": "https://sheets.google.com"
            }
        },
        evaluate_tool={
            "name": "evaluate", 
            "arguments": {
                "name": "sheet_contains",
                "text": "Q1 Sales"
            }
        }
    )
    
    agent = ClaudeAgent()
    result = await agent.run(task)
    print(f"Success: {result.reward > 0.5}")
```

## What You Can Do

### 1. Create Tasks for Browser Automation

```python
import os

task = Task(
    prompt="Navigate to GitHub and star the HUD repository",
    mcp_config={
        "hud": {
            "url": "https://mcp.hud.so/v3/mcp",
            "headers": {
                "Authorization": f"Bearer {os.getenv('HUD_API_KEY')}",
                "Mcp-Image": "hudpython/hud-browser:latest"
            }
        }
    },
    setup_tool={
        "name": "setup",
        "arguments": {
            "name": "navigate",
            "url": "https://github.com"
        }
    },
    evaluate_tool={
        "name": "evaluate", 
        "arguments": {
            "name": "url_contains",
            "substring": "hud-python"
        }
    }
)
```

### 2. Run Existing Benchmarks

```python
from datasets import load_dataset
from hud.datasets import run_dataset_parallel

# Load and run SheetBench-50 (parallel execution)
dataset = load_dataset("hud-evals/sheetbench-50", split="train")
results = await run_dataset_parallel(
    "My SheetBench Run",
    dataset,
    agent_class=ClaudeAgent
)
```

### 3. Make Your Agent Work with HUD

To create a custom agent, inherit from `MCPAgent` and implement the required methods:

```python
from hud.agents import MCPAgent
from hud.types import AgentResponse, MCPToolCall

class MyCustomAgent(MCPAgent):
    async def get_response(self, messages: list[Any]) -> AgentResponse:
        # Call your LLM and return tool calls
        ...
    
    async def format_blocks(self, blocks: list[Any]) -> list[Any]:
        # Format content blocks into messages for your LLM
        ...
    
    async def format_tool_results(
        self, tool_calls: list[MCPToolCall], 
        tool_results: list[Any]
    ) -> list[Any]:
        # Format tool results back into messages
        ...

# Now it works with any HUD dataset!
```

<Card title="Full Implementation Guide" icon="code" href="/evaluate-agents/create-agents">
  See complete examples and implementation details
</Card>

## Available Benchmarks

<CardGroup cols={2}>
<Card title="SheetBench-50" icon="table">
  50 real-world spreadsheet tasks testing data manipulation, formulas, and analysis
</Card>

<Card title="OSWorld-Verified" icon="desktop">
  Desktop automation tasks across Ubuntu applications
</Card>

<Card title="OnlineMind2Web" icon="globe">
  Web navigation and interaction challenges (Coming Soon)
</Card>

<Card title="2048 Puzzles" icon="gamepad">
  Strategic planning in the 2048 game environment
</Card>
</CardGroup>

## Exploring Environments

Use `hud analyze` to discover available tools and evaluators:

```bash
$ hud analyze hudpython/hud-browser:latest

🔍 Analyzing hudpython/hud-browser:latest...

📊 Environment Summary:
├── Tools: 15 available
├── Setup Functions: 8 available  
├── Evaluators: 12 available
└── Resources: 3 available

🛠️ Tools:
├── playwright(action: str, **kwargs) - Browser automation actions
├── click(selector: str) - Click element
├── type(selector: str, text: str) - Type text
└── ... 12 more tools

📋 Evaluators:
├── url_contains(substring: str) - Check if URL contains text
├── page_contains(text: str, regex: bool = False) - Check page content
├── element_exists(selector: str) - Check if CSS selector exists
├── todo_completed(expected_count: int) - Verify TODO completion
└── ... 8 more evaluators

Run with --json for full details or pipe to grep for filtering.
```

## Publishing to Leaderboards

After running evaluations, view results on the leaderboard:

```python
# Run evaluation (parallel)
results = await run_dataset_parallel(
    "Claude-3.5 SheetBench",
    dataset="hud-evals/sheetbench-50",
    agent_class=ClaudeAgent
)

# Then visit: app.hud.so/leaderboards/hud-evals/sheetbench-50
# Click "My Jobs" to see your runs and create scorecards
```

## Key Features

- **Reproducible**: Docker environments ensure consistency
- **Parallel**: Run multiple evaluations concurrently
- **Observable**: Every tool call tracked with telemetry
- **Extensible**: Easy to add new tasks or benchmarks

## Next Steps

<CardGroup cols={3}>
<Card title="Create Agents" icon="robot" href="/evaluate-agents/create-agents">
  Build your own MCP-compatible agent
</Card>

<Card title="Leaderboards" icon="trophy" href="/evaluate-agents/leaderboards">
  Track and compare agent performance
</Card>

<Card title="Create Benchmarks" icon="flask" href="/evaluate-agents/create-benchmarks">
  Build custom evaluation datasets
</Card>
</CardGroup>

