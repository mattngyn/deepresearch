---
title: "Agents API"
description: "API reference for HUD agent classes and methods"
icon: "robot"
---

## MCPAgent

Base class for all MCP-compatible agents.

```python
from hud.agents.base import MCPAgent
```

### Constructor

```python
MCPAgent(
    mcp_client: AgentMCPClient | None = None,
    append_setup_output: bool = True
)
```

<ParamField body="mcp_client" type="AgentMCPClient" optional>
  MCP client instance. If not provided, can be auto-created from task.mcp_config
</ParamField>

<ParamField body="append_setup_output" type="bool" default="true">
  Whether to append setup tool output to the initial prompt
</ParamField>

### Methods

#### initialize

```python
async def initialize(task: str | Task | None = None) -> None
```

Initialize the agent with optional task configuration.

<ParamField body="task" type="str | Task" optional>
  Task object or string prompt. If Task with mcp_config, auto-creates client
</ParamField>

#### run

```python
async def run(
    prompt_or_task: str | Task,
    max_steps: int = 10
) -> Trace
```

Run the agent with given prompt or task.

<ParamField body="prompt_or_task" type="str | Task" required>
  Either a string prompt or Task object with full configuration
</ParamField>

<ParamField body="max_steps" type="int" default="10">
  Maximum execution steps (-1 for unlimited)
</ParamField>

<ResponseField name="Trace" type="object">
  Execution result containing:
  - `reward`: Float score (0.0-1.0)
  - `done`: Whether task completed
  - `content`: Result content
  - `isError`: Whether error occurred
  - `trace_id`: Telemetry trace ID
  - `duration`: Execution time
  - `tool_calls`: List of tool invocations
</ResponseField>

#### fetch_response (Abstract)

```python
async def fetch_response(
    messages: List[dict],
    tools: List[dict] | None = None
) -> Tuple[List[dict], bool]
```

Core method to implement for custom agents.

<ParamField body="messages" type="List[dict]" required>
  Conversation history in OpenAI format
</ParamField>

<ParamField body="tools" type="List[dict]" optional>
  Available tools with schemas
</ParamField>

<ResponseField name="Tuple" type="(List[dict], bool)">
  - Tool calls to execute
  - Whether agent is done
</ResponseField>

#### call_tool

```python
async def call_tool(tool_call: MCPToolCall) -> MCPToolResult
```

Execute a single tool.

<ParamField body="tool_call" type="MCPToolCall" required>
  Tool name and arguments
</ParamField>

<ResponseField name="MCPToolResult" type="object">
  Tool execution result with content and error status
</ResponseField>

## ClaudeAgent

Agent using Anthropic's Claude models.

```python
from hud.agents import ClaudeAgent
```

### Constructor

```python
ClaudeAgent(
    model: str = "claude-3-5-sonnet-20241022",
    **kwargs
)
```

<ParamField body="model" type="str" default="claude-3-5-sonnet-20241022">
  Claude model to use
</ParamField>

### Environment Variables

- `ANTHROPIC_API_KEY`: Required API key

### Example

```python
agent = ClaudeAgent(model="claude-3-5-sonnet-20241022")
result = await agent.run(task)
```

## OperatorAgent

Agent using OpenAI models, including Operator.

```python
from hud.agents import OperatorAgent
```

### Constructor

```python
OperatorAgent(
    model: str = "gpt-4o",
    base_url: str | None = None,
    **kwargs
)
```

<ParamField body="model" type="str" default="gpt-4o">
  OpenAI model to use (supports Operator models)
</ParamField>

<ParamField body="base_url" type="str" optional>
  Custom API endpoint (e.g., for local models)
</ParamField>

### Environment Variables

- `OPENAI_API_KEY`: Required API key (unless using local model)

### Example

```python
# OpenAI/Operator
agent = OperatorAgent(model="gpt-4o")

# Local model with vLLM
agent = OperatorAgent(
    model="local-model",
    base_url="http://localhost:8000/v1"
)
```

## GenericOpenAIChatAgent

OpenAI-compatible chat.completions agent for custom models and endpoints.

```python
from hud.agents import GenericOpenAIChatAgent
```

### Constructor

```python
GenericOpenAIChatAgent(
    *,
    openai_client: AsyncOpenAI,
    model_name: str = "gpt-4o-mini",
    parallel_tool_calls: bool = False,
    completion_kwargs: dict[str, Any] | None = None,
    **kwargs
)
```

<ParamField body="openai_client" type="AsyncOpenAI" required>
  OpenAI-compatible client (can target OpenAI, vLLM, Ollama, etc.)
</ParamField>

<ParamField body="model_name" type="str" default="gpt-4o-mini">
  Name of the chat model to use
</ParamField>

<ParamField body="parallel_tool_calls" type="bool" default="false">
  Whether to execute multiple tool calls in a single step
</ParamField>

<ParamField body="completion_kwargs" type="dict[str, Any]" optional>
  Extra keyword arguments forwarded to `openai.chat.completions.create` (e.g., `temperature`, `top_p`, `logprobs`).
</ParamField>

### Example

```python
from openai import AsyncOpenAI

openai_client = AsyncOpenAI(
    base_url="http://localhost:8000/v1",  # Custom server
    api_key="local-key",
)

agent = GenericOpenAIChatAgent(
    openai_client=openai_client,
    model_name="custom-model",
    completion_kwargs={"temperature": 0.1, "seed": 7},
)
```

## Data Types

### Task

```python
from hud.datasets import Task

class Task(BaseModel):
    id: str | None = None
    prompt: str
    mcp_config: dict[str, Any]
    setup_tool: MCPToolCall | list[MCPToolCall] | None = None
    evaluate_tool: MCPToolCall | list[MCPToolCall] | None = None
    system_prompt: str | None = None
    metadata: dict[str, Any] = Field(default_factory=dict)
```

### MCPToolCall

```python
class MCPToolCall(BaseModel):
    name: str
    arguments: dict[str, Any] | None = None
```

### Trace

```python
class Trace(BaseModel):
    reward: float
    done: bool
    content: str
    isError: bool
    trace_id: str | None
    duration: float | None
    tool_calls: list[Any]
    trace_url: str | None
```

## Advanced Usage

### Custom Agent Implementation

```python
from hud.agents.base import MCPAgent
from typing import List, Tuple

class MyAgent(MCPAgent):
    def __init__(self, api_key: str, **kwargs):
        super().__init__(**kwargs)
        self.api_key = api_key
        self.client = MyModelClient(api_key)
    
    async def fetch_response(
        self,
        messages: List[dict],
        tools: List[dict] | None = None
    ) -> Tuple[List[dict], bool]:
        # Format messages for your model
        prompt = self.format_prompt(messages, tools)
        
        # Get model response
        response = await self.client.generate(prompt)
        
        # Parse tool calls
        tool_calls = self.parse_tools(response)
        done = self.check_done(response)
        
        return tool_calls, done
```

### Batch Processing

```python
async def run_batch(agent: MCPAgent, tasks: List[Task]):
    """Run multiple tasks concurrently"""
    results = await asyncio.gather(*[
        agent.run(task) for task in tasks
    ])
    return results
```

### With Telemetry

```python
from hud import trace

async def run_with_telemetry(agent: MCPAgent, task: Task):
    async with trace(task.id or "evaluation"):
        result = await agent.run(task)
        print(f"View at: https://app.hud.so/traces/{result.trace_id}")
        return result
```

### Error Handling

```python
try:
    result = await agent.run(task)
    if result.isError:
        print(f"Task failed: {result.content}")
    else:
        print(f"Success! Reward: {result.reward}")
except Exception as e:
    print(f"Agent error: {e}")
```

## Performance Tips

1. **Reuse agents**: Initialize once, run multiple tasks
2. **Set appropriate timeouts**: Use max_steps to prevent infinite loops
3. **Monitor token usage**: Check traces for token counts
4. **Use appropriate models**: Smaller models for simple tasks
5. **Batch similar tasks**: Process related tasks together

## Next Steps

<Card title="Environments API" icon="cube" href="/reference/api/environments">
  Environment and server API reference
</Card>
