---
title: "Tasks API"
description: "API reference for tasks, datasets, and evaluation management"
icon: "tasks"
---

## Task

Core class for defining agent evaluation scenarios.

```python
from hud.datasets import Task
```

### Constructor

```python
Task(
    id: str | None = None,
    prompt: str,
    mcp_config: dict[str, Any],
    setup_tool: MCPToolCall | list[MCPToolCall] | None = None,
    evaluate_tool: MCPToolCall | list[MCPToolCall] | None = None,
    system_prompt: str | None = None,
    metadata: dict[str, Any] = Field(default_factory=dict)
)
```

<ParamField body="id" type="str" optional>
  Unique identifier for the task
</ParamField>

<ParamField body="prompt" type="str" required>
  The instruction given to the agent
</ParamField>

<ParamField body="mcp_config" type="dict" required>
  Environment connection configuration with variable substitution support
</ParamField>

<ParamField body="setup_tool" type="MCPToolCall | list" optional>
  Tool(s) to initialize environment state
</ParamField>

<ParamField body="evaluate_tool" type="MCPToolCall | list" optional>
  Tool(s) to score agent performance
</ParamField>

<ParamField body="system_prompt" type="str" optional>
  System prompt to provide additional context to the agent
</ParamField>

<ParamField body="metadata" type="dict" optional>
  Additional task information for filtering and analysis
</ParamField>

### Environment Variable Substitution

Tasks support environment variable substitution in `mcp_config`:

```python
task = Task(
    prompt="Complete the task",
    mcp_config={
        "remote": {
            "url": "${MCP_URL:https://default.com}",
            "headers": {
                "Authorization": "Bearer ${API_KEY}"
            }
        }
    }
)
```

Format: `${VAR_NAME}` or `${VAR_NAME:default_value}`

## MCPToolCall

Defines a tool invocation.

```python
from hud.types import MCPToolCall
```

### Constructor

```python
MCPToolCall(
    name: str,
    arguments: dict[str, Any] | None = None
)
```

### Examples

```python
# Simple tool call
setup = MCPToolCall(name="reset")

# With arguments
setup = MCPToolCall(
    name="load_scenario",
    arguments={"scenario_id": "test_123"}
)

# Multiple tools
setup = [
    MCPToolCall(name="clear_data"),
    MCPToolCall(name="load_fixtures", arguments={"file": "test.json"})
]
```

## Dataset Functions

<Note>
**Recommended Evaluation Workflow:**

1. **Development:** Start with single tasks for full debugging visibility (`agent.run(task)`)
2. **Testing:** Use `run_dataset` with `max_concurrent=30` for rich CLI logs and pattern detection
3. **Production:** Use `run_dataset_parallel` for maximum speed once stable

⚠️ Multiprocess parallelization makes debugging harder due to interleaved logs. We suggest to validate with single tasks and asyncio batches before using massively parallel runners.
</Note>

### Loading Tasks from Datasets

Load tasks from HuggingFace datasets using the standard datasets library:

```python
from datasets import load_dataset
from hud.datasets import Task

# Load dataset and convert to Task objects
dataset = load_dataset("hud-evals/SheetBench-50", split="train")
tasks = [Task(**task_dict) for task_dict in dataset]

# Or work with raw dictionaries (useful for run_dataset)
raw_tasks = [dict(task) for task in dataset]
```

This approach gives you flexibility to:
- Filter tasks before converting to Task objects
- Work with raw dictionaries for `run_dataset` 
- Inspect dataset structure before processing

### save_tasks

Save tasks to HuggingFace.

```python
from hud.datasets import save_tasks

save_tasks(
    tasks: list[Task],
    repo_id: str,
    private: bool = True,
    revision: str | None = None
)
```

<ParamField body="tasks" type="list[Task]" required>
  List of Task objects to save
</ParamField>

<ParamField body="repo_id" type="str" required>
  HuggingFace repository ID (e.g., "my-org/my-tasks")
</ParamField>

<ParamField body="private" type="bool" default="true">
  Whether to make the dataset private
</ParamField>

<ParamField body="revision" type="str" optional>
  Git revision/branch name
</ParamField>

### run_dataset_parallel

Run agent on entire dataset with automatic process-based parallelization.

```python
from hud.datasets import run_dataset_parallel
from hud.agents import ClaudeAgent

# Automatic optimization
results = await run_dataset_parallel(
    name: str,
    dataset: str | Dataset | list[dict[str, Any]],
    agent_class: type[MCPAgent],
    agent_config: dict[str, Any] | None = None,
    metadata: dict[str, Any] | None = None,
    max_steps: int = 10,
    **kwargs
) -> list[Any]
```

### run_dataset_parallel_manual

Run with manual control over parallelization.

```python
from hud.datasets import run_dataset_parallel_manual

# Full control over workers and concurrency
results = await run_dataset_parallel_manual(
    name: str,
    dataset: str | Dataset | list[dict[str, Any]],
    agent_class: type[MCPAgent],
    agent_config: dict[str, Any] | None = None,
    max_workers: int | None = None,
    max_concurrent_per_worker: int = 10,
    max_concurrent: int | None = None,
    metadata: dict[str, Any] | None = None,
    max_steps: int = 10,
    split: str = "train",
    auto_respond: bool = True,
    custom_system_prompt: str | None = None
) -> list[Any]
```

<ParamField body="name" type="str" required>
  Name for the job/evaluation run
</ParamField>

<ParamField body="dataset" type="str | Dataset | list" required>
  HuggingFace dataset ID, Dataset object, or list of tasks
</ParamField>

<ParamField body="agent_class" type="type[MCPAgent]" required>
  Agent class to instantiate (e.g., ClaudeAgent)
</ParamField>

<ParamField body="agent_config" type="dict" optional>
  Configuration for agent (model, temperature, etc.)
</ParamField>

<ParamField body="max_workers" type="int | None" optional>
  Number of worker processes. Defaults to CPU count if not set.
</ParamField>

<ParamField body="max_concurrent_per_worker" type="int" default="10">
  Maximum concurrent tasks within each worker process
</ParamField>

<ParamField body="max_concurrent" type="int | None" optional>
  Total concurrent limit across all workers (overrides per-worker setting)
</ParamField>

<ParamField body="metadata" type="dict" optional>
  Metadata for the job
</ParamField>

<ParamField body="max_steps" type="int" default="40">
  Maximum steps per task
</ParamField>

<ParamField body="split" type="str" default="train">
  Dataset split when loading from string
</ParamField>

## Job Tracking

### job

Context manager for tracking evaluation jobs.

```python
from hud.telemetry import job

with job(
    name: str,
    metadata: dict[str, Any] | None = None,
    job_id: str | None = None,
    dataset_link: str | None = None
) as j:
    # Run evaluations
    results = await run_dataset_parallel(...)
```

<ParamField body="name" type="str" required>
  Human-readable job name
</ParamField>

<ParamField body="metadata" type="dict" optional>
  Additional job metadata
</ParamField>

<ParamField body="job_id" type="str" optional>
  Explicit job ID (auto-generated if not provided)
</ParamField>

<ParamField body="dataset_link" type="str" optional>
  Link to dataset source
</ParamField>

## Trace

Result of agent execution.

```python
class Trace(BaseModel):
    reward: float              # 0.0 to 1.0
    done: bool                 # Whether agent completed
    content: str              # Result content/message
    isError: bool             # Whether error occurred
    trace_id: str | None      # Telemetry trace ID
    duration: float | None    # Execution time in seconds
    tool_calls: list[Any]     # List of tool invocations
    trace_url: str | None     # URL to view trace
```

## Usage Examples

### Basic Task Creation

```python
from hud.datasets import Task
from hud.types import MCPToolCall

# Simple task
task = Task(
    prompt="Navigate to the login page",
    mcp_config={"hudpython/hud-browser:latest": {}}
)

# With setup and evaluation (using MCPToolCall)
task = Task(
    prompt="Create a spreadsheet with sales data",
    mcp_config={"hudpython/hud-browser:latest": {}},
    setup_tool=MCPToolCall(name="playwright", arguments={"action": "navigate", "url": "https://sheets.google.com"}),
    evaluate_tool=MCPToolCall(
        name="check_spreadsheet",
        arguments={"has_data": True, "has_chart": True}
    ),
    metadata={
        "category": "spreadsheet",
        "difficulty": "medium"
    }
)

# Or using simple dictionaries (recommended)
task = Task(
    prompt="Create a spreadsheet with sales data",
    mcp_config={"hudpython/hud-browser:latest": {}},
    setup_tool={"name": "playwright", "arguments": {"action": "navigate", "url": "https://sheets.google.com"}},
    evaluate_tool={
        "name": "check_spreadsheet",
        "arguments": {"has_data": True, "has_chart": True}
    },
    metadata={
        "category": "spreadsheet",
        "difficulty": "medium"
    }
)
```

### Working with Datasets

```python
from datasets import load_dataset
from hud.datasets import Task, run_dataset
from hud.agents import ClaudeAgent

# Load existing benchmark
dataset = load_dataset("hud-evals/sheetbench-50", split="train")
tasks = [Task(**task_dict) for task_dict in dataset]

# Filter tasks
medium_tasks = [
    t for t in tasks 
    if t.metadata.get("difficulty") == "medium"
]

# Run subset
results = await run_dataset_parallel(
    name="Medium Tasks Only",
    dataset=medium_tasks,  # Can pass list directly
    agent_class=ClaudeAgent,
    max_concurrent=10
)
```

### Creating Custom Datasets

```python
# Generate task variations
base_prompts = [
    "Click the submit button",
    "Fill out the contact form",
    "Navigate to settings"
]

tasks = []
for i, prompt in enumerate(base_prompts):
    for difficulty in ["easy", "medium", "hard"]:
        task = Task(
            id=f"task_{i}_{difficulty}",
            prompt=f"{prompt} (difficulty: {difficulty})",
            mcp_config={"browser-env": {}},
            metadata={
                "base_prompt": prompt,
                "difficulty": difficulty,
                "index": i
            }
        )
        tasks.append(task)

# Save to HuggingFace
save_tasks(tasks, "my-org/custom-benchmark")
```

### Batch Evaluation with Tracking

```python
from hud.telemetry import job
from hud.datasets import run_dataset

# Run with job tracking
with job("Nightly Benchmark Run", metadata={"version": "v2.1"}):
    results = await run_dataset_parallel(
        name="SheetBench Evaluation",
        dataset="hud-evals/sheetbench-50",
        agent_class=ClaudeAgent,
        agent_config={"model": "claude-3-5-sonnet-20241022"},
        max_concurrent=20
    )
    
    # Analyze results
    success_rate = sum(r.reward > 0.5 for r in results) / len(results)
    print(f"Success rate: {success_rate:.1%}")
```

### Custom Evaluation Logic

```python
# Override evaluation in task
task = Task(
    prompt="Complete the puzzle",
    mcp_config={"puzzle-env": {}},
    evaluate_tool=[
        # Multiple evaluation criteria
        MCPToolCall(name="check_complete"),
        MCPToolCall(name="check_time", arguments={"max_seconds": 120}),
        MCPToolCall(name="check_efficiency", arguments={"min_score": 0.8})
    ]
)

# Custom reward aggregation
async def evaluate_multi_criteria(agent, task):
    result = await agent.run(task)
    
    # If multiple evaluate tools, aggregate scores
    if isinstance(task.evaluate_tool, list):
        scores = []
        for tool in task.evaluate_tool:
            eval_result = await agent.call_tool(tool)
            scores.append(eval_result.reward)
        
        # Custom aggregation (e.g., weighted average)
        result.reward = sum(scores) / len(scores)
    
    return result
```

## Performance Tips

1. **Batch Size**: Adjust `max_concurrent` based on resource limits
2. **Task Caching**: Reuse loaded tasks across runs
3. **Metadata Filtering**: Use metadata for efficient task selection
4. **Environment Reuse**: Some environments support session persistence
5. **Result Storage**: Save results for later analysis

## Error Handling

```python
try:
    results = await run_dataset_parallel(
        dataset="my-tasks",
        agent_class=ClaudeAgent
    )
except Exception as e:
    print(f"Dataset run failed: {e}")
    
# Handle individual task failures
for i, result in enumerate(results):
    if result.isError:
        print(f"Task {i} failed: {result.content}")
```

## Next Steps

<CardGroup cols={2}>
<Card title="Create Tasks" icon="wrench" href="/evaluate-agents/custom-tasks">
  Build custom evaluation scenarios
</Card>

<Card title="Training Datasets" icon="database" href="/train-agents/datasets">
  Create datasets for RL training
</Card>
</CardGroup>


