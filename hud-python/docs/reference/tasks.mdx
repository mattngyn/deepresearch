---
title: "Tasks"
description: "SDK reference for task configuration and dataset utilities"
icon: "list-check"
---

The HUD SDK provides the `Task` class for defining agent objectives and dataset utilities for managing task collections.

## Task Class

```python
from hud.datasets import Task
```

Pydantic model that defines an agent's objective, setup, and evaluation criteria.

**Fields:**
| Field | Type | Description | Default |
|-------|------|-------------|---------|
| `id` | `str \| None` | Unique identifier (UUID recommended) | `None` |
| `prompt` | `str` | Task instruction for the agent | Required |
| `mcp_config` | `dict[str, Any]` | MCP server configuration | Required |
| `setup_tool` | `MCPToolCall \| list[MCPToolCall] \| None` | Tool(s) to prepare environment | `None` |
| `evaluate_tool` | `MCPToolCall \| list[MCPToolCall] \| None` | Tool(s) to score performance | `None` |
| `system_prompt` | `str \| None` | Additional system prompt | `None` |
| `metadata` | `dict[str, Any]` | Extra task metadata | `{}` |

### Environment Variable Substitution

The `mcp_config` field automatically resolves environment variables using `${VAR_NAME}` syntax:

```python
task = Task(
    prompt="Navigate to the dashboard",
    mcp_config={
        "browser": {
            "url": "${HUD_MCP_URL:https://mcp.hud.so/v3/mcp}",
            "headers": {
                "Authorization": "Bearer ${HUD_API_KEY}",
                "Mcp-Image": "hudpython/hud-browser:latest"
            }
        }
    }
)
```

**Substitution happens automatically when Task is created from a dict** - environment variables are resolved using Python's `Template.substitute()` with a defaultdict that returns empty strings for missing variables.

### Field Validators

Task automatically:

1. **Parses JSON strings** - `mcp_config` and `metadata` can be JSON strings
2. **Converts dicts to MCPToolCall** - `setup_tool` and `evaluate_tool` dicts are converted
3. **Resolves environment variables** - Only when created from dict (preserves templates in model_dump())

```python
# All of these work:
task = Task(
    prompt="Test",
    mcp_config='{"server": {"url": "..."}}',  # JSON string → dict
    setup_tool='{"name": "setup"}'            # JSON string → MCPToolCall
)

task = Task(
    prompt="Test", 
    mcp_config={...},
    setup_tool={"name": "setup", "arguments": {...}}  # dict → MCPToolCall
)

task = Task(
    prompt="Test",
    mcp_config={...},
    evaluate_tool=[  # List of tool calls
        {"name": "check_text", "arguments": {"text": "Success"}},
        {"name": "check_url", "arguments": {"url": "example.com"}}
    ]
)
```

## Recommended Evaluation Workflow

When developing and testing agents, follow this progression for optimal debugging and performance:

### Step 1: Single Task Development

Start with individual tasks to debug your agent and environment setup:

```python
# Test one task at a time for full debugging visibility
from hud.agents import ClaudeAgent
from hud.datasets import Task

task = Task(
    prompt="Click the submit button",
    mcp_config={...}
)

agent = ClaudeAgent()
result = await agent.run(task, max_steps=50)
print(f"Result: {result.reward}")
```

**Benefits:**
- Full error stack traces
- Clear log output
- Quick iteration cycle
- Easy to debug environment issues

### Step 2: Batch Testing with Asyncio

Once single tasks work reliably, test with batches using asyncio for rich logging:

```python
from hud.datasets import run_dataset

# Moderate concurrency for clear logs
results = await run_dataset(
    name="Debug Evaluation",
    dataset="hud-evals/YourDataset",
    agent_class=ClaudeAgent,
    max_concurrent=30,  # Good balance of speed and log clarity
    max_steps=50
)
```

**Benefits:**
- See all agent logs in CLI in real-time
- Catch intermittent environment issues
- Validate agent behavior at scale
- Easy to spot patterns in failures

### Step 3: Production Parallel Evaluation

After confirming stability, use parallel runners for maximum speed:

```python
from hud.datasets import run_dataset_parallel

# Fast evaluation once everything is stable
results = await run_dataset_parallel(
    name="Production Eval",
    dataset="hud-evals/Large-Dataset-1000",
    agent_class=ClaudeAgent,
    max_concurrent=50  # Prevent rate limits
)
```

**Benefits:**
- Maximum throughput (1000s of tasks)
- True process-based parallelism
- Efficient resource utilization

<Warning>
**Important:** Multiprocess parallelization makes debugging harder as logs from different processes interleave. Always validate your agent and environment with single tasks and asyncio batches before using parallel runners. This progression ensures you catch issues early when they're easiest to debug.
</Warning>

### Quick Reference

| Stage | Method | Concurrency | Use Case | Debugging |
|-------|--------|-------------|----------|-----------|
| Development | Single task | 1 | Initial debugging | Excellent |
| Testing | `run_dataset` | 30 | Validate at scale | Good |
| Production | `run_dataset_parallel` | 1000s | Fast evaluation | Limited |

## Dataset Functions

### run_dataset_parallel (Automatic)

```python
from hud.datasets import run_dataset_parallel

# Automatic optimization - recommended for most cases
results = await run_dataset_parallel(
    name="My Evaluation",
    dataset="hud-evals/SheetBench-50",  # or Dataset object or list[dict]
    agent_class=ClaudeAgent,
    agent_config={"model": "claude-3-5-sonnet"},
    max_steps=50
    # Automatically optimizes worker processes based on CPU cores
)
```

Run all tasks with automatic process-based parallelization.

### run_dataset_parallel_manual (Full Control)

```python
from hud.datasets import run_dataset_parallel_manual

# Manual control over parallelization
results = await run_dataset_parallel_manual(
    name="My Evaluation",
    dataset="hud-evals/SheetBench-50",
    agent_class=ClaudeAgent,
    agent_config={"model": "claude-3-5-sonnet"},
    max_workers=8,                # 8 worker processes
    max_concurrent_per_worker=10, # 10 concurrent tasks per worker
    max_steps=50
)

# Or with total concurrent limit
results = await run_dataset_parallel_manual(
    name="Rate Limited Eval",
    dataset=dataset,
    agent_class=ClaudeAgent,
    max_workers=8,
    max_concurrent=20,  # Max 20 total concurrent (overrides per-worker)
    max_steps=50
)
```

Run all tasks with manual control over parallelization settings.

**Manual Parallelism Control Parameters:**
| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `max_workers` | `int \| None` | Number of worker processes | `None` (auto) |
| `max_concurrent_per_worker` | `int` | Concurrent tasks within each worker | `10` |
| `max_concurrent` | `int \| None` | Total concurrent limit (overrides per-worker) | `None` |

**Parameters:**
| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `name` | `str` | Job name for tracking | Required |
| `dataset` | `str \| Dataset \| list[dict]` | HF dataset ID, Dataset object, or task dicts | Required |
| `agent_class` | `type[MCPAgent]` | Agent class to instantiate | Required |
| `agent_config` | `dict[str, Any] \| None` | Constructor kwargs for agent | `None` |

| `metadata` | `dict[str, Any] \| None` | Job metadata | `None` |
| `max_steps` | `int` | Max steps per task | `40` |
| `split` | `str` | Dataset split when loading by ID | `"train"` |
| `auto_respond` | `bool` | Use ResponseAgent for continuations | `False` |
| `custom_system_prompt` | `str \| None` | Override dataset system prompt | `None` |

**Returns:** `list[Trace]` - Results in dataset order

**Features:**
- Automatic job tracking with `hud.job()`
- System prompt loading from dataset's `system_prompt.txt`
- Parallel execution with semaphore control
- Progress tracking with individual traces
- Graceful error handling (continues on failures)

**Example:**
```python
# From HuggingFace dataset ID
results = await run_dataset_parallel(
    "SheetBench Evaluation",
    "hud-evals/SheetBench-50",
    ClaudeAgent,
    {"model": "claude-3-5-sonnet-20241022"}
)

# From Dataset object
from datasets import load_dataset
dataset = load_dataset("hud-evals/OSWorld-Verified", split="train")
results = await run_dataset_parallel("OSWorld Test", dataset, OperatorAgent)

# From list of dicts
tasks = [
    {
        "prompt": "Click submit",
        "mcp_config": {...},
        "setup_tool": {"name": "setup"}
    }
]
results = await run_dataset_parallel("Custom", tasks, ClaudeAgent)
```

### fetch_system_prompt_from_dataset

```python
from hud.datasets import fetch_system_prompt_from_dataset

system_prompt = await fetch_system_prompt_from_dataset("hud-evals/SheetBench-50")
```

Fetch `system_prompt.txt` from a HuggingFace dataset repository.

**Returns:** `str | None` - System prompt text if found

**Note:** Requires `huggingface_hub` to be installed.

### save_tasks  

```python
from hud.datasets import save_tasks

# IMPORTANT: Pass dicts, not Task objects!
task_dicts = [task.model_dump() for task in tasks]
save_tasks(
    tasks=task_dicts,
    repo_id="my-org/my-tasks",
    private=False,
    dataset_name="My Tasks v1.0"
)
```

Save tasks to HuggingFace dataset with JSON string serialization.

**Parameters:**
| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `tasks` | `list[dict[str, Any]]` | Task dictionaries (NOT Task objects) | Required |
| `repo_id` | `str` | HuggingFace repository ID | Required |
| `**kwargs` | `Any` | Additional args for `push_to_hub()` | - |

**Important:** Always pass dictionaries to preserve environment variable templates:

```python
# ✅ GOOD - Preserves ${HUD_API_KEY} in the dataset
task_dicts = [task.model_dump() for task in tasks]
save_tasks(task_dicts, "my-dataset")

# ❌ BAD - Would expose resolved API keys!
# save_tasks(tasks, "my-dataset")  # Don't do this!
```

**Dataset Schema:**
The function converts complex fields to JSON strings for clean HuggingFace schema:
- `mcp_config` → JSON string
- `setup_tool` → JSON string (if present)
- `evaluate_tool` → JSON string (if present)
- `metadata` → JSON string (if present)

## MCPToolCall Type

```python
from hud.types import MCPToolCall
```

Defines a tool invocation with arguments.

**Fields:**
| Field | Type | Description | Default |
|-------|------|-------------|---------|
| `name` | `str` | Tool name to call | Required |
| `arguments` | `dict[str, Any]` | Tool arguments | `{}` |

## Real-World Examples

### Loading Tasks from Datasets

From `examples/run_evaluation.py`:

```python
# Load dataset
dataset = load_dataset("hud-evals/SheetBench-50", split="train")

# Get system prompt
dataset_system_prompt = await fetch_system_prompt_from_dataset("hud-evals/SheetBench-50")

# Run single task
sample_task = dataset[0]
task = Task(**sample_task)  # Auto-converts dict to Task

# Or run all tasks
results = await run_dataset_parallel(
    "Full SheetBench Evaluation",
    dataset,
    ClaudeAgent,
    {"model": "claude-3-5-sonnet-20241022"},
    max_concurrent=50
)
```

### Task Structure in Datasets

From `environments/text_2048/2048_taskconfigs.json`:

```json
{
    "id": "2048_target_256",
    "prompt": "You are playing 2048 and your goal is to reach the 256 tile.",
    "mcp_config": {
        "local": {
            "command": "docker",
            "args": ["run", "--rm", "-i", "hud-text-2048"]
        }
    },
    "setup_tool": {
        "name": "setup",
        "arguments": {
            "name": "board",
            "arguments": {"board_size": 4}
        }
    },
    "evaluate_tool": {
        "name": "evaluate", 
        "arguments": {
            "name": "max_number",
            "arguments": {"target": 256}
        }
    },
    "metadata": {
        "difficulty": 256,
        "game": "2048"
    }
}
```

### Creating and Saving Tasks

```python
import os
from uuid import uuid4

# Set environment variables
os.environ["HUD_API_KEY"] = "sk-hud-..."
os.environ["BROWSER_PROVIDER"] = "anchorbrowser"

# Create tasks with env var templates
tasks = []
for url in ["example.com", "test.com"]:
    task = Task(
        id=str(uuid4()),
        prompt=f"Navigate to {url} and find the login button",
        mcp_config={
            "browser": {
                "url": "${HUD_MCP_URL:https://mcp.hud.so/v3/mcp}",
                "headers": {
                    "Authorization": "Bearer ${HUD_API_KEY}",
                    "X-Provider": "${BROWSER_PROVIDER}"
                }
            }
        },
        setup_tool=MCPToolCall(
            name="setup",
            arguments={"name": "navigate", "arguments": {"url": url}}
        ),
        evaluate_tool=MCPToolCall(
            name="evaluate", 
            arguments={"name": "element_exists", "arguments": {"selector": "button.login"}}
        ),
        metadata={
            "category": "navigation",
            "difficulty": "easy"
        }
    )
    tasks.append(task)

# Save to HuggingFace (preserves env var templates)
task_dicts = [t.model_dump() for t in tasks]
save_tasks(task_dicts, "my-org/navigation-tasks")
```

### Agent Integration

Tasks automatically configure agents:

```python
task = Task(
    prompt="Complete the form",
    mcp_config={...},
    setup_tool={...},
    evaluate_tool={...},
    system_prompt="Be very careful with form fields"
)

agent = ClaudeAgent()

# When agent.run(task) is called:
# 1. If no mcp_client, creates one from task.mcp_config
# 2. Adds task.system_prompt to agent's system prompt
# 3. Adds lifecycle tools to agent.lifecycle_tools
# 4. Runs setup_tool (hidden from agent)
# 5. Executes task.prompt
# 6. Runs evaluate_tool (hidden from agent)
# 7. Returns Trace with evaluation reward
```

## Best Practices

1. **Use UUIDs for task IDs** - Required for HuggingFace datasets
2. **Save dictionaries, not objects** - Preserves env var templates
3. **Include system_prompt.txt** - Upload to dataset repo root
4. **Use metadata for filtering** - Category, difficulty, tags
5. **Test locally first** - Before uploading to HuggingFace
6. **Version your datasets** - Use meaningful repo names

## Common Patterns

### Filtering Tasks

```python
# Load all tasks
dataset = load_dataset("hud-evals/WebArena-Lite", split="train")

# Filter in Python
login_tasks = [
    task for task in dataset
    if "login" in task["prompt"].lower()
]

# Convert and run
results = await run_dataset_parallel(
    "Login Tasks Only",
    login_tasks,
    ClaudeAgent
)
```

### Custom System Prompts

```python
# Override dataset system prompt
results = await run_dataset_parallel(
    "Custom Evaluation",
    "hud-evals/SheetBench-50",
    ClaudeAgent,
    custom_system_prompt="You are an expert spreadsheet user. Be precise."
)
```

### Environment Variable Management

```python
# Tasks preserve templates
task = Task(
    prompt="Test",
    mcp_config={
        "server": {
            "api_key": "${API_KEY:default-key}"
        }
    }
)

# model_dump() preserves the template
data = task.model_dump()
assert data["mcp_config"]["server"]["api_key"] == "${API_KEY:default-key}"

# But task object has resolved value
os.environ["API_KEY"] = "real-key"
task2 = Task(**data)
# task2.mcp_config["server"]["api_key"] is now "real-key"
```

## See Also

- [Task System](/core-concepts/task-system) - Conceptual overview
- [Create Benchmarks](/evaluate-agents/create-benchmarks) - Building datasets
- [Agents](/reference/agents) - How agents use tasks